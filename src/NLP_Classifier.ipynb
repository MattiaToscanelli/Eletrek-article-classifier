{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to write dataframe to csv file\n",
    "def saveToCsv(fileName, df):\n",
    "    with open(f'../data/{fileName}.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        solarWriter = csv.writer(f)\n",
    "        solarWriter.writerow(df.columns)\n",
    "        solarWriter.writerows(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from csv files\n",
    "solar = pd.read_csv('../data/solar.csv')\n",
    "ebikes = pd.read_csv('../data/ebikes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3436\\151283968.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ebikes['content'] = ebikes['content'].str.replace(r\"\\n\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3436\\151283968.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ebikes['content'] = ebikes['content'].str.replace(r\"\\r\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3436\\151283968.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  solar['content'] = solar['content'].str.replace(r\"\\n\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3436\\151283968.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  solar['content'] = solar['content'].str.replace(r\"\\r\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# Remove new line characters\n",
    "ebikes['content'] = ebikes['content'].str.replace(r\"\\n\", \"\")\n",
    "ebikes['content'] = ebikes['content'].str.replace(r\"\\r\", \"\")\n",
    "solar['content'] = solar['content'].str.replace(r\"\\n\", \"\")\n",
    "solar['content'] = solar['content'].str.replace(r\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to data\n",
    "solar['label'] = 'solar'\n",
    "ebikes['label'] = 'ebikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(solar) = 1204, len(ebikes) = 1032\n",
      "len(solar) = 1032, len(ebikes) = 1032\n"
     ]
    }
   ],
   "source": [
    "# Print lenght of solar and length of ebikes datasets\n",
    "print(f'len(solar) = {len(solar)}, len(ebikes) = {len(ebikes)}')\n",
    "\n",
    "# Take first 1032 rows of solar dataset (to have same number of rows as ebikes dataset)\n",
    "solar = solar.head(1032)\n",
    "print(f'len(solar) = {len(solar)}, len(ebikes) = {len(ebikes)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% for training and 20% for testing)\n",
    "solar_train, solar_test = train_test_split(solar, test_size=0.2, random_state=42)\n",
    "ebikes_train, ebikes_test = train_test_split(ebikes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the solar and ebikes sets for train and test\n",
    "train = pd.concat([solar_train, ebikes_train]).reset_index(drop=True)\n",
    "test = pd.concat([solar_test, ebikes_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      0.99      1.00       207\n",
      "       solar       0.99      1.00      1.00       207\n",
      "\n",
      "    accuracy                           1.00       414\n",
      "   macro avg       1.00      1.00      1.00       414\n",
      "weighted avg       1.00      1.00      1.00       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the content and label columns from the dataframes\n",
    "train_data = train[\"content\"].values\n",
    "train_labels = train[\"label\"].values\n",
    "test_data = test[\"content\"].values\n",
    "test_labels = test[\"label\"].values\n",
    "\n",
    "# Conversione of texts into features\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data)\n",
    "test_features = count_vect.transform(test_data)\n",
    "\n",
    "# Training \n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_counts, train_labels)\n",
    "\n",
    "# Test\n",
    "predictions = classifier.predict(test_features)\n",
    "\n",
    "target_names = ['ebikes', 'solar']\n",
    "print(classification_report(test_labels, predictions, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Classification based on the number of occurrencies of the name of the category itself or of it's minor variations, i.e. plural forms or different ways of spelling it (e.g. 'bike' / 'ebike' / 'e-bike' / 'bicycle' / 'e-bicycle')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Creates and returns the vocabulary of a given document\n",
    "def create_vocabulary(document, remove_stop_words=False, remove_punctuation=False, remove_numbers=False, remove_duplicates=False, docLanguage='english'):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(document, language=docLanguage)\n",
    "    stop_words = set(nltk.corpus.stopwords.words(docLanguage)) if remove_stop_words else []\n",
    "    punctuation = set(string.punctuation) if remove_punctuation else []\n",
    "\n",
    "    vocabulary = [t.lower() for t in tokens \n",
    "                  if not ((t.lower() in stop_words)\n",
    "                  or (t.lower() in punctuation)\n",
    "                  or (t.lower().isdigit() and remove_numbers))]\n",
    "    \n",
    "    if remove_duplicates:\n",
    "        return list(set(vocabulary))\n",
    "    else:\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a text based on its vocabulary into one of the given classes\n",
    "def classify_text(text, classes):\n",
    "    vocabulary = create_vocabulary(text, True, True, True)\n",
    "\n",
    "    class_counts = {}\n",
    "    for cl in classes:\n",
    "        class_counts[cl] = 0\n",
    "\n",
    "    for word in vocabulary:\n",
    "        for cl in classes:\n",
    "            if word in classes[cl]:\n",
    "                class_counts[cl] += 1\n",
    "\n",
    "    return max(class_counts, key=class_counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the given texts into the given classes and evaluate the results with the main classification metrics\n",
    "def classify_texts_and_evaluate(texts, classes):\n",
    "    texts['baseline'] = texts['content'].apply(lambda x: classify_text(x, classes))\n",
    "    texts['baseline'].value_counts(normalize=True)\n",
    "    \n",
    "    print(classification_report(texts['label'].values, texts['baseline'].values, target_names=sorted(classes.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% for training and 20% for testing)\n",
    "solar_train, solar_test = train_test_split(solar, test_size=0.2, random_state=42)\n",
    "ebikes_train, ebikes_test = train_test_split(ebikes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the solar and ebikes sets for train and test\n",
    "train = pd.concat([solar_train, ebikes_train]).reset_index(drop=True)\n",
    "test = pd.concat([solar_test, ebikes_test]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "classes = {'solar': ['panels', 'panel'], 'ebikes': ['ebike', 'ebikes', 'bike', 'bikes', 'e-bike', 'e-bikes', 'bicycle', 'bicycles', 'e-bicycle', 'e-bicycles']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      0.33      0.50         3\n",
      "       solar       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.75      0.67      0.58         5\n",
      "weighted avg       0.80      0.60      0.57         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying to classify some dummy texts with the baseline approach\n",
    "dummy_texts = pd.DataFrame({\n",
    "    'content': ['I like solar panels', 'A friend of mine just bought a really expensive e-bike', 'I like e-bicycles but I like solar panels too, because with a solar panel I can recharge all my electric gadgets.', 'I want to be self-sufficient and respect the environment.', 'My two-wheels is so cool!'], \n",
    "    'label': ['solar', 'ebikes', 'ebikes', 'solar', 'ebikes']})\n",
    "classify_texts_and_evaluate(dummy_texts, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      1.00      1.00       207\n",
      "       solar       1.00      1.00      1.00       207\n",
      "\n",
      "    accuracy                           1.00       414\n",
      "   macro avg       1.00      1.00      1.00       414\n",
      "weighted avg       1.00      1.00      1.00       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with the real test set\n",
    "classify_texts_and_evaluate(test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2020/03/31</td>\n",
       "      <td>In today’s Electrek Green Energy Brief (EGEB):...</td>\n",
       "      <td>solar</td>\n",
       "      <td>ebikes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                            content  label  \\\n",
       "86  2020/03/31  In today’s Electrek Green Energy Brief (EGEB):...  solar   \n",
       "\n",
       "   baseline  \n",
       "86   ebikes  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the texts that were misclassified\n",
    "test.loc[test['baseline'] != test['label']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

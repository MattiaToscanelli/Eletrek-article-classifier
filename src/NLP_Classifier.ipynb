{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to write dataframe to csv file\n",
    "def saveToCsv(fileName, df):\n",
    "    with open(f'../data/{fileName}.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        solarWriter = csv.writer(f)\n",
    "        solarWriter.writerow(df.columns)\n",
    "        solarWriter.writerows(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from csv files\n",
    "solar = pd.read_csv('../data/solar.csv')\n",
    "ebikes = pd.read_csv('../data/ebikes.csv')\n",
    "tesla = pd.read_csv('../data/tesla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ebikes['content'] = ebikes['content'].str.replace(r\"\\n\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ebikes['content'] = ebikes['content'].str.replace(r\"\\r\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  solar['content'] = solar['content'].str.replace(r\"\\n\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  solar['content'] = solar['content'].str.replace(r\"\\r\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tesla['content'] = tesla['content'].str.replace(r\"\\n\", \"\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_19620\\1301546108.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tesla['content'] = tesla['content'].str.replace(r\"\\r\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# Remove new line characters\n",
    "ebikes['content'] = ebikes['content'].str.replace(r\"\\n\", \"\")\n",
    "ebikes['content'] = ebikes['content'].str.replace(r\"\\r\", \"\")\n",
    "solar['content'] = solar['content'].str.replace(r\"\\n\", \"\")\n",
    "solar['content'] = solar['content'].str.replace(r\"\\r\", \"\")\n",
    "tesla['content'] = tesla['content'].str.replace(r\"\\n\", \"\")\n",
    "tesla['content'] = tesla['content'].str.replace(r\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to data\n",
    "solar['label'] = 'solar'\n",
    "ebikes['label'] = 'ebikes'\n",
    "tesla['label'] = 'tesla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(solar) = 1204, len(ebikes) = 1032, len(tesla) = 1202\n",
      "len(solar) = 1032, len(ebikes) = 1032, len(tesla) = 1032\n"
     ]
    }
   ],
   "source": [
    "# Print lenght of the datasets\n",
    "print(f'len(solar) = {len(solar)}, len(ebikes) = {len(ebikes)}, len(tesla) = {len(tesla)}')\n",
    "\n",
    "# Take first 1032 rows of the solar and Tesla datasets (to have the same number of rows as the ebikes dataset)\n",
    "solar = solar.head(1032)\n",
    "tesla = tesla.head(1032)\n",
    "print(f'len(solar) = {len(solar)}, len(ebikes) = {len(ebikes)}, len(tesla) = {len(tesla)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% for training and 20% for testing)\n",
    "solar_train, solar_test = train_test_split(solar, test_size=0.2, random_state=42)\n",
    "ebikes_train, ebikes_test = train_test_split(ebikes, test_size=0.2, random_state=42)\n",
    "tesla_train, tesla_test = train_test_split(tesla, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the sets for train and test\n",
    "train = pd.concat([solar_train, ebikes_train, tesla_train]).reset_index(drop=True)\n",
    "test = pd.concat([solar_test, ebikes_test, tesla_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      0.99      1.00       207\n",
      "       solar       0.97      1.00      0.98       207\n",
      "       tesla       1.00      0.98      0.99       207\n",
      "\n",
      "    accuracy                           0.99       621\n",
      "   macro avg       0.99      0.99      0.99       621\n",
      "weighted avg       0.99      0.99      0.99       621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the content and label columns from the dataframes\n",
    "train_data = train[\"content\"].values\n",
    "train_labels = train[\"label\"].values\n",
    "test_data = test[\"content\"].values\n",
    "test_labels = test[\"label\"].values\n",
    "\n",
    "# Conversione of texts into features\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data)\n",
    "test_features = count_vect.transform(test_data)\n",
    "\n",
    "# Training \n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_counts, train_labels)\n",
    "\n",
    "# Test\n",
    "predictions = classifier.predict(test_features)\n",
    "\n",
    "target_names = ['ebikes', 'solar', 'tesla']\n",
    "print(classification_report(test_labels, predictions, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Classification based on the number of occurrencies of the name of the category itself or of it's minor variations, i.e. plural forms or different ways of spelling it (e.g. 'bike' / 'ebike' / 'e-bike' / 'bicycle' / 'e-bicycle')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Creates and returns the vocabulary of a given document\n",
    "def create_vocabulary(document, remove_stop_words=False, remove_punctuation=False, remove_numbers=False, remove_duplicates=False, docLanguage='english'):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(document, language=docLanguage)\n",
    "    stop_words = set(nltk.corpus.stopwords.words(docLanguage)) if remove_stop_words else []\n",
    "    punctuation = set(string.punctuation) if remove_punctuation else []\n",
    "\n",
    "    vocabulary = [t.lower() for t in tokens \n",
    "                  if not ((t.lower() in stop_words)\n",
    "                  or (t.lower() in punctuation)\n",
    "                  or (t.lower().isdigit() and remove_numbers))]\n",
    "    \n",
    "    if remove_duplicates:\n",
    "        return list(set(vocabulary))\n",
    "    else:\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a text based on its vocabulary into one of the given classes\n",
    "def classify_text(text, classes):\n",
    "    vocabulary = create_vocabulary(text, True, True, True)\n",
    "\n",
    "    class_counts = {}\n",
    "    for cl in classes:\n",
    "        class_counts[cl] = 0\n",
    "\n",
    "    for word in vocabulary:\n",
    "        for cl in classes:\n",
    "            if word in classes[cl]:\n",
    "                class_counts[cl] += 1\n",
    "\n",
    "    return max(class_counts, key=class_counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the given texts into the given classes and evaluate the results with the main classification metrics\n",
    "def classify_texts_and_evaluate(texts, classes):\n",
    "    texts['baseline'] = texts['content'].apply(lambda x: classify_text(x, classes))\n",
    "    texts['baseline'].value_counts(normalize=True)\n",
    "    \n",
    "    print(classification_report(texts['label'].values, texts['baseline'].values, target_names=sorted(classes.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% for training and 20% for testing)\n",
    "solar_train, solar_test = train_test_split(solar, test_size=0.2, random_state=42)\n",
    "ebikes_train, ebikes_test = train_test_split(ebikes, test_size=0.2, random_state=42)\n",
    "tesla_train, tesla_test = train_test_split(tesla, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the sets for train and test\n",
    "train = pd.concat([solar_train, ebikes_train, tesla_train]).reset_index(drop=True)\n",
    "test = pd.concat([solar_test, ebikes_test, tesla_test]).reset_index(drop=True)\n",
    "\n",
    "# Define the classes and their keywords\n",
    "classes = {'solar': ['panels', 'panel'], 'ebikes': ['ebike', 'ebikes', 'bike', 'bikes', 'e-bike', 'e-bikes', 'bicycle', 'bicycles', 'e-bicycle', 'e-bicycles'],  'tesla': ['tesla', 'model', 'elon', 'musk']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      0.33      0.50         3\n",
      "       solar       0.50      1.00      0.67         2\n",
      "       tesla       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.83      0.78      0.72         8\n",
      "weighted avg       0.88      0.75      0.73         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying to classify some dummy texts with the baseline approach\n",
    "dummy_texts = pd.DataFrame({\n",
    "    'content': ['I like solar panels', 'A friend of mine just bought a really expensive e-bike', 'I like e-bicycles but I like solar panels too, because with a solar panel I can recharge all my electric gadgets.', 'I want to be self-sufficient and respect the environment.', 'My two-wheels is so cool!', 'Elon Musk is the best', 'I want to buy a Tesla', 'I tried the Model 3 and it was amazing'], \n",
    "    'label': ['solar', 'ebikes', 'ebikes', 'solar', 'ebikes', 'tesla', 'tesla', 'tesla']})\n",
    "classify_texts_and_evaluate(dummy_texts, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ebikes       1.00      1.00      1.00       207\n",
      "       solar       0.93      0.56      0.70       207\n",
      "       tesla       0.69      0.96      0.80       207\n",
      "\n",
      "    accuracy                           0.84       621\n",
      "   macro avg       0.87      0.84      0.83       621\n",
      "weighted avg       0.87      0.84      0.83       621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with the real test set\n",
    "classify_texts_and_evaluate(test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/11/21</td>\n",
       "      <td>Quaise Energy is on a mission to prove that de...</td>\n",
       "      <td>solar</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/12/29</td>\n",
       "      <td>The world’s first floating offshore wind farm,...</td>\n",
       "      <td>solar</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023/03/16</td>\n",
       "      <td>Tesla is launching a new feature to help homeo...</td>\n",
       "      <td>solar</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023/03/16</td>\n",
       "      <td>Tesla is launching a new feature to help homeo...</td>\n",
       "      <td>solar</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022/01/19</td>\n",
       "      <td>The US Department of Interior’s Bureau of Ocea...</td>\n",
       "      <td>solar</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2023/03/23</td>\n",
       "      <td>Lucid Motors is in the process of delivering a...</td>\n",
       "      <td>tesla</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023/03/23</td>\n",
       "      <td>Lucid Motors is in the process of delivering a...</td>\n",
       "      <td>tesla</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2023/03/23</td>\n",
       "      <td>Lucid Motors is in the process of delivering a...</td>\n",
       "      <td>tesla</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>2023/03/23</td>\n",
       "      <td>Lucid Motors is in the process of delivering a...</td>\n",
       "      <td>tesla</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>2023/03/23</td>\n",
       "      <td>Lucid Motors is in the process of delivering a...</td>\n",
       "      <td>tesla</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                            content  label  \\\n",
       "0    2022/11/21  Quaise Energy is on a mission to prove that de...  solar   \n",
       "1    2022/12/29  The world’s first floating offshore wind farm,...  solar   \n",
       "4    2023/03/16  Tesla is launching a new feature to help homeo...  solar   \n",
       "5    2023/03/16  Tesla is launching a new feature to help homeo...  solar   \n",
       "6    2022/01/19  The US Department of Interior’s Bureau of Ocea...  solar   \n",
       "..          ...                                                ...    ...   \n",
       "490  2023/03/23  Lucid Motors is in the process of delivering a...  tesla   \n",
       "499  2023/03/23  Lucid Motors is in the process of delivering a...  tesla   \n",
       "546  2023/03/23  Lucid Motors is in the process of delivering a...  tesla   \n",
       "582  2023/03/23  Lucid Motors is in the process of delivering a...  tesla   \n",
       "593  2023/03/23  Lucid Motors is in the process of delivering a...  tesla   \n",
       "\n",
       "    baseline  \n",
       "0      tesla  \n",
       "1      tesla  \n",
       "4      tesla  \n",
       "5      tesla  \n",
       "6      tesla  \n",
       "..       ...  \n",
       "490    solar  \n",
       "499    solar  \n",
       "546    solar  \n",
       "582    solar  \n",
       "593    solar  \n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the texts that were misclassified\n",
    "test.loc[test['baseline'] != test['label']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
